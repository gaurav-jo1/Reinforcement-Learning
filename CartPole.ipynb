{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKWxTgofn/i0TI48YIE/b6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gaurav-jo1/Reinforcement-Learning/blob/main/CartPole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7VVnr4upRPJ"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key components:\n",
        "\n",
        "* `Agent:` The decision-maker (e.g., a robot, game player).\n",
        "* `Environment:` The world the agent interacts with (e.g., a game, a simulation).\n",
        "* `State (s):` The current situation of the environment.\n",
        "* `Action (a):` What the agent can do.\n",
        "* `Reward (r):` Feedback from the environment after an action.\n",
        "* `Policy (Ï€):` The strategy mapping states to actions.\n",
        "* `Value Function:` Estimates how good a state or action is in terms of future rewards."
      ],
      "metadata": {
        "id": "KeRw64KdwgfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GAMMA = 0.99  # Discount factor\n",
        "EPSILON = 1.0  # Exploration rate\n",
        "EPSILON_MIN = 0.01\n",
        "EPSILON_DECAY = 0.995\n",
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 32\n",
        "MEMORY_SIZE = 10000\n",
        "EPISODES = 500"
      ],
      "metadata": {
        "id": "OjWGknEXqwsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "  def __init__(self, state_size, action_size):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(state_size, 64)\n",
        "    self.fc2 = nn.Linear(64, 64)\n",
        "    self.fc3 = nn.Linear(64, action_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.relu(self.fc1(x))\n",
        "    x = torch.relu(self.fc2(x))\n",
        "    return self.fc3(x)"
      ],
      "metadata": {
        "id": "l8Xa3toXqxtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayMemory:\n",
        "  def __init__(self, capacity):\n",
        "    self.memory = deque(maxlen=capacity)\n",
        "\n",
        "  def push(self, transition):\n",
        "    self.memory.append(transition)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    return random.sample(self.memory, batch_size)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.memory)"
      ],
      "metadata": {
        "id": "e15cpw3tW5gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent():\n",
        "  def __init__(self, state_size, action_size):\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    self.memory = ReplayMemory(MEMORY_SIZE)\n",
        "    self.model = DQN(state_size, action_size)\n",
        "    self.optimizer = optim.Adam(self.model.parameters(), lr=LEARNING_RATE)\n",
        "    self.epsilon = EPSILON\n",
        "\n",
        "  def select_action(self, state):\n",
        "    if random.random() < self.epsilon:\n",
        "      return random.randrange(self.action_size)\n",
        "    with torch.no_grad():\n",
        "      state = torch.FloatTensor(state).unsqueeze(0)\n",
        "      q_values = self.model(state)\n",
        "      return q_values.argmax().item()\n",
        "\n",
        "  def train(self):\n",
        "    if len(self.memory) < BATCH_SIZE:\n",
        "      return\n",
        "\n",
        "    transitions = self.memory.sample(BATCH_SIZE)\n",
        "    batch = list(zip(*transitions))\n",
        "\n",
        "    states = torch.FloatTensor(np.array(batch[0]))\n",
        "    actions = torch.LongTensor(batch[1])\n",
        "    rewards = torch.FloatTensor(batch[2])\n",
        "    next_states = torch.FloatTensor(np.array(batch[3]))\n",
        "    dones = torch.FloatTensor(batch[4])\n",
        "\n",
        "    q_values = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "    next_q_values = self.model(next_states).max(1)[0]\n",
        "\n",
        "    targets = rewards + (GAMMA * next_q_values * (1 - drones))\n",
        "\n",
        "    loss = nn.MSELoss()(q_values, targets.detach())\n",
        "\n",
        "  def update_epsilon(self):\n",
        "    self.epsilon = max(EPSILON_MIN, self.epsilon * EPSILON_DECAY)\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "agent = DQNAgent(state_size, action_size)\n",
        "scores = []\n",
        "\n",
        "for episode in range(EPISODES):\n",
        "  state, _ = env.reset()\n",
        "  total_reward = 0\n",
        "  done = False\n",
        "\n",
        "  while not done:\n",
        "    action = agent.select_action(state)\n",
        "        next_state, reward, done, truncated, _ = env.step(action)\n",
        "        done = done or truncated\n",
        "        total_reward += reward\n",
        "\n",
        "        # Store transition\n",
        "        agent.memory.push((state, action, reward, next_state, float(done)))\n",
        "        agent.train()\n",
        "        state = next_state\n",
        "\n",
        "    agent.update_epsilon()\n",
        "    scores.append(total_reward)\n",
        "    if episode % 10 == 0:\n",
        "      print(f\"Episode {episode}, Avg Reward: {np.mean(scores[-10:])}\")\n",
        "\n",
        "env.close()\n",
        "\n",
        "plt.plot(scores)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "loCZq-vmVRop"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}